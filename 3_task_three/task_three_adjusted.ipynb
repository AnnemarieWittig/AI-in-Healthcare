{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. After you performed a pre-processing (data cleaning, feature normalization and/or selection, â€¦) on them, apply one machine learning technique of your choice to the pre-processed data to solve your research question.\n",
    "2. If necessary, perform hyperparameters tuning.\n",
    "3. Once you obtain you best model, determine the most important factors that\n",
    "contributed to your results (explainability).\n",
    "\n",
    "#### Approach\n",
    "\n",
    "I am choosing to address my 2nd research question \"Can we predict the gender of similarly sick patients based on the different burdening of the feet when walking (regarding sensor values under the feet)?\". Taking my notes from task one as a reference: For answering my own questions, I propose to enhance the data from the demographics dataset with the averages and median of each of the signal features of the respective patients. Additionally, I'd like to add the minimal and maximal values as well as a min-max range (max-min) as I think these provide some insight in the extreme values or peaks and a summary measure of the overall variation or spread in the burdening of the feet. Overall, these four features should nicely display all relevant information in a overseeable way. As it is not completely clear to me what the numbering of the signal files means, I will only use _01 files but might run do a separate run with all to see if it makes a difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read imputed file from task 1 into a pandas dataframe using tabs as separator\n",
    "#df = pd.read_csv('../task_one/imputed_data_knn.csv', sep=',')\n",
    "df = pd.read_csv('../task_one/imputed_data_mean.csv', sep=',')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-processing of the base dataset:  \n",
    "We start by doing the mapping of the signal files onto the correct ID. Then we read out the signal file and calculate: \n",
    "- mean\n",
    "- median\n",
    "- min\n",
    "- max\n",
    "- min-max range\n",
    "\n",
    "using the corresponding dataframe. These columns are appended to the original df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'GaPt15': ['GaPt15_01.txt', 'GaPt15_02.txt'], 'JuPt03': ['JuPt03_01.txt', 'JuPt03_02.txt', 'JuPt03_03.txt', 'JuPt03_04.txt', 'JuPt03_05.txt', 'JuPt03_06.txt', 'JuPt03_07.txt'], 'JuPt10': ['JuPt10_01.txt', 'JuPt10_02.txt', 'JuPt10_03.txt', 'JuPt10_04.txt', 'JuPt10_05.txt', 'JuPt10_06.txt', 'JuPt10_07.txt'], 'GaPt31': ['GaPt31_01.txt', 'GaPt31_02.txt'], 'GaPt32': ['GaPt32_01.txt', 'GaPt32_02.txt'], 'JuPt01': ['JuPt01_01.txt', 'JuPt01_02.txt', 'JuPt01_03.txt', 'JuPt01_04.txt', 'JuPt01_05.txt', 'JuPt01_06.txt'], 'SiPt08': ['SiPt08_01.txt'], 'JuPt28': ['JuPt28_01.txt', 'JuPt28_02.txt', 'JuPt28_03.txt', 'JuPt28_04.txt', 'JuPt28_05.txt', 'JuPt28_06.txt', 'JuPt28_07.txt'], 'GaCo09': ['GaCo09_01.txt', 'GaCo09_02.txt'], 'GaCo10': ['GaCo10_01.txt', 'GaCo10_02.txt'], 'SiPt07': ['SiPt07_01.txt'], 'GaPt22': ['GaPt22_01.txt', 'GaPt22_02.txt'], 'GaPt16': ['GaPt16_01.txt', 'GaPt16_02.txt'], 'JuCo14': ['JuCo14_01.txt'], 'GaPt18': ['GaPt18_01.txt', 'GaPt18_02.txt'], 'SiCo01': ['SiCo01_01.txt'], 'JuPt29': ['JuPt29_01.txt', 'JuPt29_02.txt', 'JuPt29_03.txt', 'JuPt29_04.txt', 'JuPt29_05.txt', 'JuPt29_06.txt', 'JuPt29_07.txt'], 'GaPt28': ['GaPt28_01.txt', 'GaPt28_02.txt'], 'JuPt06': ['JuPt06_01.txt', 'JuPt06_02.txt', 'JuPt06_03.txt', 'JuPt06_04.txt', 'JuPt06_05.txt', 'JuPt06_06.txt', 'JuPt06_07.txt'], 'SiPt33': ['SiPt33_01.txt'], 'GaPt26': ['GaPt26_01.txt', 'GaPt26_02.txt'], 'GaPt03': ['GaPt03_01.txt'], 'GaCo04': ['GaCo04_01.txt', 'GaCo04_02.txt'], 'GaPt07': ['GaPt07_01.txt', 'GaPt07_02.txt'], 'GaPt06': ['GaPt06_01.txt'], 'JuPt14': ['JuPt14_01.txt'], 'GaPt25': ['GaPt25_01.txt', 'GaPt25_02.txt'], 'GaPt13': ['GaPt13_01.txt', 'GaPt13_02.txt'], 'JuPt20': ['JuPt20_01.txt', 'JuPt20_02.txt', 'JuPt20_03.txt', 'JuPt20_04.txt', 'JuPt20_05.txt', 'JuPt20_06.txt', 'JuPt20_07.txt'], 'GaCo16': ['GaCo16_01.txt', 'GaCo16_02.txt'], 'JuPt15': ['JuPt15_01.txt', 'JuPt15_02.txt', 'JuPt15_03.txt', 'JuPt15_04.txt', 'JuPt15_05.txt', 'JuPt15_06.txt', 'JuPt15_07.txt'], 'JuPt23': ['JuPt23_01.txt', 'JuPt23_02.txt', 'JuPt23_03.txt', 'JuPt23_04.txt', 'JuPt23_05.txt', 'JuPt23_06.txt', 'JuPt23_07.txt'], 'JuPt22': ['JuPt22_01.txt'], 'JuPt24': ['JuPt24_01.txt', 'JuPt24_02.txt'], 'JuPt21': ['JuPt21_01.txt', 'JuPt21_02.txt', 'JuPt21_03.txt', 'JuPt21_04.txt', 'JuPt21_05.txt', 'JuPt21_06.txt', 'JuPt21_07.txt'], 'JuPt09': ['JuPt09_01.txt', 'JuPt09_02.txt', 'JuPt09_03.txt', 'JuPt09_04.txt', 'JuPt09_05.txt'], 'SiPt21': ['SiPt21_01.txt'], 'GaPt21': ['GaPt21_01.txt', 'GaPt21_02.txt'], 'SiPt15': ['SiPt15_01.txt'], 'GaCo06': ['GaCo06_01.txt', 'GaCo06_02.txt'], 'GaCo07': ['GaCo07_01.txt', 'GaCo07_02.txt'], 'SiCo17': ['SiCo17_01.txt'], 'SiPt29': ['SiPt29_01.txt'], 'JuPt11': ['JuPt11_01.txt', 'JuPt11_02.txt', 'JuPt11_03.txt', 'JuPt11_04.txt', 'JuPt11_05.txt', 'JuPt11_06.txt', 'JuPt11_07.txt'], 'JuPt26': ['JuPt26_01.txt'], 'SiCo06': ['SiCo06_01.txt'], 'JuPt18': ['JuPt18_01.txt'], 'GaPt23': ['GaPt23_01.txt', 'GaPt23_02.txt'], 'SiCo20': ['SiCo20_01.txt'], 'GaPt27': ['GaPt27_01.txt', 'GaPt27_02.txt'], 'SiPt36': ['SiPt36_01.txt'], 'GaPt12': ['GaPt12_01.txt', 'GaPt12_02.txt'], 'JuCo06': ['JuCo06_01.txt'], 'JuCo15': ['JuCo15_01.txt'], 'GaCo17': ['GaCo17_01.txt', 'GaCo17_02.txt'], 'SiCo04': ['SiCo04_01.txt'], 'GaCo03': ['GaCo03_01.txt', 'GaCo03_02.txt'], 'GaCo14': ['GaCo14_01.txt', 'GaCo14_02.txt'], 'JuPt07': ['JuPt07_01.txt'], 'GaPt08': ['GaPt08_01.txt', 'GaPt08_02.txt'], 'SiPt02': ['SiPt02_01.txt'], 'JuCo24': ['JuCo24_01.txt'], 'GaPt29': ['GaPt29_01.txt', 'GaPt29_02.txt'], 'GaCo05': ['GaCo05_01.txt', 'GaCo05_02.txt'], 'GaCo02': ['GaCo02_01.txt', 'GaCo02_02.txt'], 'JuCo18': ['JuCo18_01.txt'], 'SiCo22': ['SiCo22_01.txt'], 'JuCo08': ['JuCo08_01.txt'], 'SiCo27': ['SiCo27_01.txt'], 'JuCo21': ['JuCo21_01.txt'], 'JuCo03': ['JuCo03_01.txt'], 'SiCo24': ['SiCo24_01.txt'], 'SiPt24': ['SiPt24_01.txt'], 'SiCo12': ['SiCo12_01.txt'], 'JuCo01': ['JuCo01_01.txt'], 'SiCo15': ['SiCo15_01.txt'], 'SiPt31': ['SiPt31_01.txt'], 'JuCo16': ['JuCo16_01.txt'], 'GaPt14': ['GaPt14_01.txt', 'GaPt14_02.txt'], 'SiCo03': ['SiCo03_01.txt'], 'GaPt19': ['GaPt19_01.txt', 'GaPt19_02.txt'], 'JuPt04': ['JuPt04_01.txt'], 'SiCo30': ['SiCo30_01.txt'], 'GaPt33': ['GaPt33_01.txt', 'GaPt33_02.txt'], 'SiPt14': ['SiPt14_01.txt'], 'GaCo15': ['GaCo15_01.txt', 'GaCo15_02.txt'], 'SiPt32': ['SiPt32_01.txt'], 'JuPt08': ['JuPt08_01.txt'], 'JuCo11': ['JuCo11_01.txt'], 'GaCo13': ['GaCo13_01.txt', 'GaCo13_02.txt'], 'SiCo19': ['SiCo19_01.txt'], 'SiPt05': ['SiPt05_01.txt'], 'GaPt20': ['GaPt20_01.txt', 'GaPt20_02.txt'], 'GaPt24': ['GaPt24_01.txt', 'GaPt24_02.txt'], 'JuPt05': ['JuPt05_01.txt'], 'GaCo08': ['GaCo08_01.txt', 'GaCo08_02.txt'], 'GaPt05': ['GaPt05_01.txt'], 'SiPt30': ['SiPt30_01.txt'], 'SiCo21': ['SiCo21_01.txt'], 'SiPt40': ['SiPt40_01.txt'], 'JuPt16': ['JuPt16_01.txt'], 'SiCo13': ['SiCo13_01.txt'], 'SiPt13': ['SiPt13_01.txt'], 'JuPt17': ['JuPt17_01.txt'], 'JuCo26': ['JuCo26_01.txt'], 'GaPt30': ['GaPt30_01.txt', 'GaPt30_02.txt'], 'GaPt17': ['GaPt17_01.txt', 'GaPt17_02.txt'], 'SiPt23': ['SiPt23_01.txt'], 'SiPt20': ['SiPt20_01.txt'], 'JuPt02': ['JuPt02_01.txt'], 'GaCo12': ['GaCo12_01.txt'], 'SiPt12': ['SiPt12_01.txt'], 'JuPt25': ['JuPt25_01.txt'], 'JuPt19': ['JuPt19_01.txt'], 'GaCo11': ['GaCo11_01.txt'], 'SiPt16': ['SiPt16_01.txt'], 'JuCo09': ['JuCo09_01.txt'], 'SiPt28': ['SiPt28_01.txt'], 'SiCo23': ['SiCo23_01.txt'], 'SiCo05': ['SiCo05_01.txt'], 'SiCo09': ['SiCo09_01.txt'], 'SiPt04': ['SiPt04_01.txt'], 'JuPt13': ['JuPt13_01.txt'], 'SiPt34': ['SiPt34_01.txt'], 'GaCo01': ['GaCo01_01.txt'], 'SiCo26': ['SiCo26_01.txt'], 'JuCo07': ['JuCo07_01.txt'], 'SiCo14': ['SiCo14_01.txt'], 'SiCo07': ['SiCo07_01.txt'], 'SiPt18': ['SiPt18_01.txt'], 'SiPt37': ['SiPt37_01.txt'], 'SiCo25': ['SiCo25_01.txt'], 'SiCo08': ['SiCo08_01.txt'], 'JuPt12': ['JuPt12_01.txt'], 'JuCo02': ['JuCo02_01.txt'], 'SiPt25': ['SiPt25_01.txt'], 'SiPt39': ['SiPt39_01.txt'], 'GaPt04': ['GaPt04_01.txt'], 'JuCo25': ['JuCo25_01.txt'], 'SiCo29': ['SiCo29_01.txt'], 'SiCo18': ['SiCo18_01.txt'], 'JuCo22': ['JuCo22_01.txt'], 'SiPt10': ['SiPt10_01.txt'], 'SiPt09': ['SiPt09_01.txt'], 'SiPt19': ['SiPt19_01.txt'], 'SiPt17': ['SiPt17_01.txt'], 'JuCo20': ['JuCo20_01.txt'], 'SiCo11': ['SiCo11_01.txt'], 'SiPt38': ['SiPt38_01.txt'], 'GaPt09': ['GaPt09_01.txt', 'GaPt09_02.txt'], 'SiPt35': ['SiPt35_01.txt'], 'JuPt27': ['JuPt27_01.txt'], 'JuCo12': ['JuCo12_01.txt'], 'GaCo22': ['GaCo22_01.txt'], 'JuCo23': ['JuCo23_01.txt'], 'JuCo13': ['JuCo13_01.txt'], 'JuCo05': ['JuCo05_01.txt'], 'JuCo17': ['JuCo17_01.txt'], 'SiPt27': ['SiPt27_01.txt'], 'SiPt22': ['SiPt22_01.txt'], 'JuCo04': ['JuCo04_01.txt'], 'SiCo28': ['SiCo28_01.txt'], 'SiCo16': ['SiCo16_01.txt'], 'JuCo19': ['JuCo19_01.txt'], 'SiCo10': ['SiCo10_01.txt']}\n"
     ]
    }
   ],
   "source": [
    "# Getting the file mapping from task one in case we want to move to using all files\n",
    "import os\n",
    "\n",
    "directory = '../dataset/'\n",
    "\n",
    "# Get a list of all files in the directory\n",
    "files = os.listdir(directory)\n",
    "connected_files = {}\n",
    "\n",
    "for file in files:\n",
    "    if not 'txt' in file:\n",
    "        continue\n",
    "    data_id = file.split('_')[0]\n",
    "    connected_file = []\n",
    "\n",
    "    # Map connected files to each other\n",
    "    counter = 1\n",
    "    while True:\n",
    "        formatted_counter = \"{:02d}\".format(counter)\n",
    "        built_file = f\"{data_id}_{formatted_counter}.txt\"\n",
    "        if built_file in files:\n",
    "            connected_file.append(built_file)\n",
    "        else:\n",
    "            connected_files[data_id] = connected_file\n",
    "            break\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "print(connected_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ID Study  Group  Subjnum  Gender  Age  Height     Weight  HoehnYahr  \\\n",
      "0  GaPt03    Ga      1        3       2   82    1.45  50.000000        3.0   \n",
      "1  GaPt04    Ga      1        4       1   68    1.71  72.558282        2.5   \n",
      "2  GaPt05    Ga      1        5       2   82    1.53  51.000000        2.5   \n",
      "3  GaPt06    Ga      1        6       1   72    1.70  82.000000        2.0   \n",
      "4  GaPt07    Ga      1        7       2   53    1.67  54.000000        3.0   \n",
      "\n",
      "   UPDRS  ...  Total Force L_MEAN  Total Force L_MEDIAN  Total Force L_MIN  \\\n",
      "0   20.0  ...            6.868664              7.495400                0.0   \n",
      "1   25.0  ...            6.408688              7.231428                0.0   \n",
      "2   24.0  ...            8.064273              9.636863                0.0   \n",
      "3   16.0  ...            6.799862              8.054146                0.0   \n",
      "4   44.0  ...            5.916058              6.714074                0.0   \n",
      "\n",
      "   Total Force L_MAX  Total Force L_MIN-MAX-RANGE  Total Force R_MEAN  \\\n",
      "0          14.887400                    14.887400            7.295230   \n",
      "1          13.610851                    13.610851            6.728295   \n",
      "2          17.628039                    17.628039            7.692027   \n",
      "3          14.620610                    14.620610            6.410452   \n",
      "4          13.735741                    13.735741            6.468458   \n",
      "\n",
      "   Total Force R_MEDIAN  Total Force R_MIN  Total Force R_MAX  \\\n",
      "0              7.900200                0.0          15.708000   \n",
      "1              7.795389                0.0          13.901928   \n",
      "2              8.994118                0.0          16.551765   \n",
      "3              7.344512                0.0          14.084024   \n",
      "4              7.948519                0.0          14.640185   \n",
      "\n",
      "   Total Force R_MIN-MAX-RANGE  \n",
      "0                    15.708000  \n",
      "1                    13.901928  \n",
      "2                    16.551765  \n",
      "3                    14.084024  \n",
      "4                    14.640185  \n",
      "\n",
      "[5 rows x 110 columns]\n"
     ]
    }
   ],
   "source": [
    "df = df[df[\"Group\"] == 1.0]\n",
    "\n",
    "# Mapping the files to the dataframe\n",
    "columns = ['Time', 'VGRF L1', 'VGRF L2', 'VGRF L3', 'VGRF L4', 'VGRF L5', 'VGRF L6', 'VGRF L7', 'VGRF L8', 'VGRF R1', 'VGRF R2', 'VGRF R3', 'VGRF R4', 'VGRF R5', 'VGRF R6', 'VGRF R7', 'VGRF R8', 'Total Force L', 'Total Force R']\n",
    "rows_to_remove = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    id = row['ID']\n",
    "    if id not in connected_files:\n",
    "        rows_to_remove.append(index)\n",
    "        continue\n",
    "\n",
    "    list = connected_files[id]\n",
    "    if list == []: \n",
    "        print( f'NO FILE FOR ID {id}')\n",
    "        rows_to_remove.append(index)\n",
    "        continue\n",
    "\n",
    "    weight = row['Weight']\n",
    "\n",
    "    first_file = list[0]\n",
    "    signal_df = pd.read_csv(directory + first_file, sep=\"\\t\", header=None)\n",
    "    signal_df.columns = columns\n",
    "    signal_df = signal_df.drop(columns=[\"Time\"])\n",
    "\n",
    "    signal_df = signal_df.astype(float)\n",
    "\n",
    "    mean_values = signal_df.mean()\n",
    "    median_values = signal_df.median()\n",
    "    min_values = signal_df.min()\n",
    "    max_values = signal_df.max()\n",
    "\n",
    "    for column in signal_df.columns:\n",
    "        df.loc[index, f'{column}_MEAN'] = mean_values[column] / weight\n",
    "        df.loc[index, f'{column}_MEDIAN'] = median_values[column] / weight\n",
    "        df.loc[index, f'{column}_MIN'] = min_values[column] / weight\n",
    "        df.loc[index, f'{column}_MAX'] = max_values[column] / weight\n",
    "        df.loc[index, f'{column}_MIN-MAX-RANGE'] = (max_values[column] - min_values[column]) / weight\n",
    "\n",
    "for index in rows_to_remove:\n",
    "    df = df.drop(index)\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-processing of the base dataset 2nd step:  \n",
    "Since the question focuses on sick patients, I already dropped the healthy ones before doing the first pre-processing step to save some space.\n",
    "Now we select the features we need / drop the ones we do not need by their column name. For that, features that are not included in the research question / needed for the answer should be dropped. E.g. keeping height and weight might cause wrong conclusions as men averagely might be taller than women. Subjnum, ID, Study are organisational in nature and can be removed, too. Lastly, the group can be dropped as we only have sick patients left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Gender', 'HoehnYahr', 'UPDRS', 'UPDRSM', 'TUAG', 'VGRF L1_MEAN',\n",
      "       'VGRF L1_MEDIAN', 'VGRF L1_MIN', 'VGRF L1_MAX', 'VGRF L1_MIN-MAX-RANGE',\n",
      "       'VGRF L2_MEAN', 'VGRF L2_MEDIAN', 'VGRF L2_MIN', 'VGRF L2_MAX',\n",
      "       'VGRF L2_MIN-MAX-RANGE', 'VGRF L3_MEAN', 'VGRF L3_MEDIAN',\n",
      "       'VGRF L3_MIN', 'VGRF L3_MAX', 'VGRF L3_MIN-MAX-RANGE', 'VGRF L4_MEAN',\n",
      "       'VGRF L4_MEDIAN', 'VGRF L4_MIN', 'VGRF L4_MAX', 'VGRF L4_MIN-MAX-RANGE',\n",
      "       'VGRF L5_MEAN', 'VGRF L5_MEDIAN', 'VGRF L5_MIN', 'VGRF L5_MAX',\n",
      "       'VGRF L5_MIN-MAX-RANGE', 'VGRF L6_MEAN', 'VGRF L6_MEDIAN',\n",
      "       'VGRF L6_MIN', 'VGRF L6_MAX', 'VGRF L6_MIN-MAX-RANGE', 'VGRF L7_MEAN',\n",
      "       'VGRF L7_MEDIAN', 'VGRF L7_MIN', 'VGRF L7_MAX', 'VGRF L7_MIN-MAX-RANGE',\n",
      "       'VGRF L8_MEAN', 'VGRF L8_MEDIAN', 'VGRF L8_MIN', 'VGRF L8_MAX',\n",
      "       'VGRF L8_MIN-MAX-RANGE', 'VGRF R1_MEAN', 'VGRF R1_MEDIAN',\n",
      "       'VGRF R1_MIN', 'VGRF R1_MAX', 'VGRF R1_MIN-MAX-RANGE', 'VGRF R2_MEAN',\n",
      "       'VGRF R2_MEDIAN', 'VGRF R2_MIN', 'VGRF R2_MAX', 'VGRF R2_MIN-MAX-RANGE',\n",
      "       'VGRF R3_MEAN', 'VGRF R3_MEDIAN', 'VGRF R3_MIN', 'VGRF R3_MAX',\n",
      "       'VGRF R3_MIN-MAX-RANGE', 'VGRF R4_MEAN', 'VGRF R4_MEDIAN',\n",
      "       'VGRF R4_MIN', 'VGRF R4_MAX', 'VGRF R4_MIN-MAX-RANGE', 'VGRF R5_MEAN',\n",
      "       'VGRF R5_MEDIAN', 'VGRF R5_MIN', 'VGRF R5_MAX', 'VGRF R5_MIN-MAX-RANGE',\n",
      "       'VGRF R6_MEAN', 'VGRF R6_MEDIAN', 'VGRF R6_MIN', 'VGRF R6_MAX',\n",
      "       'VGRF R6_MIN-MAX-RANGE', 'VGRF R7_MEAN', 'VGRF R7_MEDIAN',\n",
      "       'VGRF R7_MIN', 'VGRF R7_MAX', 'VGRF R7_MIN-MAX-RANGE', 'VGRF R8_MEAN',\n",
      "       'VGRF R8_MEDIAN', 'VGRF R8_MIN', 'VGRF R8_MAX', 'VGRF R8_MIN-MAX-RANGE',\n",
      "       'Total Force L_MEAN', 'Total Force L_MEDIAN', 'Total Force L_MIN',\n",
      "       'Total Force L_MAX', 'Total Force L_MIN-MAX-RANGE',\n",
      "       'Total Force R_MEAN', 'Total Force R_MEDIAN', 'Total Force R_MIN',\n",
      "       'Total Force R_MAX', 'Total Force R_MIN-MAX-RANGE'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Drop organisational\n",
    "df = df.drop(columns=[\"Group\", \"ID\", \"Study\", \"Subjnum\"])\n",
    "\n",
    "# Drop general\n",
    "df = df.drop(columns=[\"Age\", \"Weight\", \"Height\"])\n",
    "\n",
    "# Drop speeds\n",
    "df = df.drop(columns=['Speed_01','Speed_02','Speed_03','Speed_04','Speed_05','Speed_06','Speed_07','Speed_10'])\n",
    "\n",
    "print(df.columns)\n",
    "df.to_csv('adjusted_prepped_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total datasize: 93\n",
      "For fold 1 testing data starts at (including) 0 and ends with (excluding) 18\n",
      "For fold 2 testing data starts at (including) 18 and ends with (excluding) 36\n",
      "For fold 3 testing data starts at (including) 36 and ends with (excluding) 54\n",
      "For fold 4 testing data starts at (including) 54 and ends with (excluding) 72\n",
      "For fold 5 testing data starts at (including) 72 and ends with (excluding) 93\n",
      "(Meaned) Mean Accuracy on training data: 0.882000  \n",
      "(Meaned) F1-score on training data: 0.907971  \n",
      "(Meaned) Mean Accuracy on test data: 0.646032  \n",
      "(Meaned) F1-score on test data: 0.698371  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "folds = 5\n",
    "\n",
    "df_copy = df.copy()\n",
    "\n",
    "df_copy = df_copy.sample(frac=1)\n",
    "df_copy = df_copy.reset_index(drop=True)\n",
    "\n",
    "df_y = df_copy['Gender']\n",
    "df_x = df_copy.drop(columns=[\"Gender\"])\n",
    "\n",
    "acc_train = 0\n",
    "acc_test = 0\n",
    "f1_train = 0\n",
    "f1_test = 0\n",
    "\n",
    "print(f'Total datasize: {len(df_copy)}')\n",
    "for fold in range(folds):\n",
    "    fold_size = int(len(df) / folds)\n",
    "    start = int(fold * fold_size)\n",
    "    end = int((fold+1) * fold_size)\n",
    "\n",
    "    # use \"left-over\" data (from rounding) in the last fold\n",
    "    if fold == (folds - 1):\n",
    "        end = len(df)\n",
    "\n",
    "    print(f'For fold {fold + 1} testing data starts at (including) {start} and ends with (excluding) {end}')\n",
    "\n",
    "    df_x_train = pd.concat([df_x[:start], df_x[end:]])\n",
    "    df_y_train = pd.concat([df_y[:start], df_y[end:]])\n",
    "\n",
    "    df_x_test = df_x[start:end]\n",
    "    df_y_test = df_y[start:end]\n",
    "\n",
    "    regression = LogisticRegression(max_iter=6000)\n",
    "\n",
    "    regression.fit(df_x_train, df_y_train)\n",
    "    \n",
    "    y_pred_train = regression.predict(df_x_train)\n",
    "    y_pred_test = regression.predict(df_x_test)\n",
    "    f1_score_train = f1_score(df_y_train, y_pred_train)\n",
    "    f1_score_test = f1_score(df_y_test, y_pred_test)\n",
    "\n",
    "    acc_train += regression.score(df_x_train, df_y_train)\n",
    "    f1_train += f1_score_train\n",
    "    acc_test += regression.score(df_x_test, df_y_test)\n",
    "    f1_test += f1_score_test\n",
    "\n",
    "print(\"(Meaned) Mean Accuracy on training data: %f  \" % (acc_train / folds))\n",
    "print(\"(Meaned) F1-score on training data: %f  \" % (f1_train / folds))\n",
    "print(\"(Meaned) Mean Accuracy on test data: %f  \" % (acc_test / folds))\n",
    "print(\"(Meaned) F1-score on test data: %f  \" % (f1_test / folds))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using knn-imputed dataset:\n",
    "This section is only to note down results from the x-fold-crossvalidation of the previous cell\n",
    "\n",
    "#### Filtered by sick patients only\n",
    "Dataset after filtering: 93 rows;  \n",
    "\n",
    "5 folds, 6000 iterations;  \n",
    "(Meaned) Mean Accuracy on training data: 0.892556  \n",
    "(Meaned) F1-score on training data: 0.916099  \n",
    "(Meaned) Mean Accuracy on test data: 0.620635  \n",
    "(Meaned) F1-score on test data: 0.708771  \n",
    "\n",
    "3 folds, 6000 iterations;  \n",
    "(Meaned) Mean Accuracy on training data: 0.913978  \n",
    "(Meaned) F1-score on training data: 0.932373  \n",
    "(Meaned) Mean Accuracy on test data: 0.666667  \n",
    "(Meaned) F1-score on test data: 0.734908  \n",
    "\n",
    "2 folds, 6000 iterations;  \n",
    "(Meaned) Mean Accuracy on training data: 0.935476  \n",
    "(Meaned) F1-score on training data: 0.948621  \n",
    "(Meaned) Mean Accuracy on test data: 0.666512  \n",
    "(Meaned) F1-score on test data: 0.717049  \n",
    "\n",
    "#### All Patients \n",
    "Full dataset; \n",
    "\n",
    "5 folds (average size of a test fold here equals the test fold of 3 fold filtered), 6000 iterations  \n",
    "(Meaned) Mean Accuracy on training data: 0.866667  \n",
    "(Meaned) F1-score on training data: 0.889698  \n",
    "(Meaned) Mean Accuracy on test data: 0.715152  \n",
    "(Meaned) F1-score on test data: 0.756155  \n",
    "\n",
    "3 folds, 6000 iterations   \n",
    "(Meaned) Mean Accuracy on training data: 0.869697  \n",
    "(Meaned) F1-score on training data: 0.891870  \n",
    "(Meaned) Mean Accuracy on test data: 0.721212  \n",
    "(Meaned) F1-score on test data: 0.759351  \n",
    "\n",
    "2 folds, 6000 iterations  \n",
    "(Meaned) Mean Accuracy on training data: 0.878416  \n",
    "(Meaned) F1-score on training data: 0.900869  \n",
    "(Meaned) Mean Accuracy on test data: 0.745151  \n",
    "(Meaned) F1-score on test data: 0.794482  \n",
    "\n",
    "It is to be mentioned that sklearn warns / advises (sometimes, sometimes not) to use a higher number of iterations for the bigger dataset. However, to keep the environment the same in each testrun, I am running these on the same as previous tests.\n",
    "\n",
    "### Using mean-imputed dataset:\n",
    "This section is only to note down results from the x-fold-crossvalidation of the previous cell\n",
    "\n",
    "#### Filtered by sick patients only\n",
    "Dataset after filtering: 93 rows;  \n",
    "\n",
    "5 folds, 6000 iterations;  \n",
    "(Meaned) Mean Accuracy on training data: 0.882000  \n",
    "(Meaned) F1-score on training data: 0.907971  \n",
    "(Meaned) Mean Accuracy on test data: 0.646032  \n",
    "(Meaned) F1-score on test data: 0.698371 \n",
    "\n",
    "3 folds, 6000 iterations;  \n",
    "(Meaned) Mean Accuracy on training data: 1.000000  \n",
    "(Meaned) F1-score on training data: 1.000000  \n",
    "(Meaned) Mean Accuracy on test data: 0.677419  \n",
    "(Meaned) F1-score on test data: 0.743888  \n",
    "\n",
    "2 folds, 6000 iterations;  \n",
    "(Meaned) Mean Accuracy on training data: 1.000000  \n",
    "(Meaned) F1-score on training data: 1.000000  \n",
    "(Meaned) Mean Accuracy on test data: 0.612165  \n",
    "(Meaned) F1-score on test data: 0.677512  \n",
    "\n",
    "#### All Patients \n",
    "Full dataset; \n",
    "\n",
    "5 folds (average size of a test fold here equals the test fold of 3 fold filtered), 6000 iterations  \n",
    "(Meaned) Mean Accuracy on training data: 0.854545  \n",
    "(Meaned) F1-score on training data: 0.880594  \n",
    "(Meaned) Mean Accuracy on test data: 0.715152  \n",
    "(Meaned) F1-score on test data: 0.764899  \n",
    "\n",
    "3 folds, 6000 iterations   \n",
    "(Meaned) Mean Accuracy on training data: 0.860606  \n",
    "(Meaned) F1-score on training data: 0.886573  \n",
    "(Meaned) Mean Accuracy on test data: 0.684848  \n",
    "(Meaned) F1-score on test data: 0.735721  \n",
    "\n",
    "2 folds, 6000 iterations  \n",
    "(Meaned) Mean Accuracy on training data: 0.933074  \n",
    "(Meaned) F1-score on training data: 0.947846  \n",
    "(Meaned) Mean Accuracy on test data: 0.709080  \n",
    "(Meaned) F1-score on test data: 0.764706  \n",
    "\n",
    "It is to be mentioned that sklearn warns / advises (sometimes, sometimes not) to use a higher number of iterations for the bigger dataset. However, to keep the environment the same in each testrun, I am running these on the same as previous tests."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
